{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "114f66453afe594",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%m/%d/%Y %H:%M:%S'\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "log.info(\"Log initialized.\")"
   ],
   "id": "6d67329fe40b714d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.data_loader import MINDDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = os.getenv(\"HOME\") + \"/data/MINDsmall\"\n",
    "train_ds = MINDDataset(data_dir, split='train')\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4)"
   ],
   "id": "36c1d1a2f45e4c4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils.model import GPTRec\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "model = GPTRec(vocab_size=len(train_ds.nid2idx)).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "id": "6d56064522ad32db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for epoch in range(5):\n",
    "    tot_loss, tot_acc, tot_samples = 0, 0, 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    for batch in pbar:\n",
    "        # Filter out the data with positive samples in batch\n",
    "        pos_hist, pos_target = [], []\n",
    "        for h, imp, lab in zip(batch['history'], batch['impressions'], batch['labels']):\n",
    "            if sum(lab) > 0:\n",
    "                pos_idx = lab.index(1)\n",
    "                pos_hist.append(h)\n",
    "                pos_target.append(imp[pos_idx])\n",
    "        if len(pos_hist) == 0:\n",
    "            continue\n",
    "\n",
    "        # Construct input_ids, mask, labels\n",
    "        # input_ids: [CLS] history [SEP]\n",
    "        # all sequences are padded to the same length\n",
    "        max_len = max(len(h) for h in pos_hist)\n",
    "        input_ids = torch.zeros(len(pos_hist), max_len + 2, dtype=torch.long)\n",
    "        for i, h in enumerate(pos_hist):\n",
    "            input_ids[i, 0] = model.cls_token_id\n",
    "            input_ids[i, 1:1+len(h)] = torch.tensor(h, dtype=torch.long)\n",
    "            input_ids[i, 1+len(h)] = model.sep_token_id\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        mask = (input_ids != 0).to(torch.long).to(device)\n",
    "        labels = torch.tensor(pos_target, dtype=torch.long).to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask=mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        acc = (pred == labels).float().sum().item()\n",
    "\n",
    "        tot_loss += loss.item() * len(pos_hist)\n",
    "        tot_acc += acc\n",
    "        tot_samples += len(pos_hist)\n",
    "\n",
    "        pbar.set_postfix(loss=tot_loss/tot_samples, acc=tot_acc/tot_samples)\n",
    "\n",
    "    log.info(f\"Epoch {epoch}  Loss {tot_loss/tot_samples:.4f}  Acc {tot_acc/tot_samples:.4f}\")\n",
    "\n",
    "log.info(\"Training finished. Saving model...\")\n",
    "torch.save(model.state_dict(), \"baseline.pt\")\n",
    "log.info(\"Model saved to baseline.pt\")"
   ],
   "id": "47af82147ece567b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Eval TODO",
   "id": "6fbe828fd6334f77",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
