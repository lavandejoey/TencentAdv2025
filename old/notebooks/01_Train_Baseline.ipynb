{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTRec Baseline on MIND-small\n",
    "No Azure â€” all local files under `~/data/MINDsmall`."
   ],
   "id": "d0281b29ad83bcc"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T17:45:35.743915Z",
     "start_time": "2025-06-30T17:45:35.734657Z"
    }
   },
   "source": [
    "# Imports\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "import math\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%m/%d/%Y %H:%M:%S'\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "log.info(\"Log initialized.\")\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.data_loader import MINDDataset\n",
    "from utils.embeddings import load_vec\n",
    "from utils.encoders import TextImageEncoder\n",
    "from utils.model import GPTRec\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "log.info(\"Device: %s\", device)\n",
    "data_root = Path(os.environ[\"HOME\"]) / \"data\" / \"MINDsmall\""
   ],
   "id": "3762c3ec8722321c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2025 19:45:35 - INFO - Log initialized.\n",
      "06/30/2025 19:45:35 - INFO - Device: cuda\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T17:45:36.486859Z",
     "start_time": "2025-06-30T17:45:35.769440Z"
    }
   },
   "source": [
    "# load embeddings\n",
    "ent_mat, ent2idx = load_vec(str(data_root / \"train\" / \"entity_embedding.vec\"))\n",
    "rel_mat, rel2idx = load_vec(str(data_root / \"train\" / \"relation_embedding.vec\"))\n",
    "log.info(\"Embeddings shapes - entities: %s, relations: %s\", ent_mat.shape, rel_mat.shape)"
   ],
   "id": "b8c3a0a3c7b73dfe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/30/2025 19:45:36 - INFO - Embeddings shapes - entities: (26904, 100), relations: (1091, 100)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T17:45:57.875949Z",
     "start_time": "2025-06-30T17:45:36.509680Z"
    }
   },
   "source": [
    "# load dataset\n",
    "def collate_fn(batch):\n",
    "    H = pad_sequence([b[\"history\"]] for b in batch], batch_first=True, padding_value=0)\n",
    "    I = pad_sequence([b[\"impr_ids\"]] for b in batch], batch_first=True, padding_value=0)\n",
    "    L = pad_sequence([b[\"impr_lbls\"]] for b in batch], batch_first=True, padding_value=0)\n",
    "    return {\"history\": H, \"impr_ids\": I, \"impr_lbls\": L}\n",
    "\n",
    "\n",
    "ds_train = MINDDataset(str(data_root), split=\"train\")\n",
    "ds_val = MINDDataset(str(data_root), split=\"dev\")\n",
    "loader_train = DataLoader(ds_train, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "loader_val = DataLoader(ds_val, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=4)"
   ],
   "id": "7a120c68e8d1e523",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T17:46:04.015137Z",
     "start_time": "2025-06-30T17:45:57.882318Z"
    }
   },
   "source": [
    "# test encoder\n",
    "enc = TextImageEncoder(device=device, max_len=32)\n",
    "batch = next(iter(loader_train))\n",
    "txt = enc.encode_text([\n",
    "    ds_train.news.iloc[i][\"title\"]\n",
    "    for i in batch[\"history\"][0].tolist()\n",
    "])\n",
    "log.info(\"Text features shape: %s\", txt.shape)"
   ],
   "id": "32a31bb2e08eaa30",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zliu/miniconda3/envs/adv312/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zliu/miniconda3/envs/adv312/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_1016175/3085113961.py\", line 4, in collate_fn\n    I = torch.stack([b[\"impr_ids\"] for b in batch])\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [5] at entry 0 and [13] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# test encoder\u001B[39;00m\n\u001B[32m      2\u001B[39m enc = TextImageEncoder(device=device, max_len=\u001B[32m32\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m batch = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mloader_train\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m txt = enc.encode_text([\n\u001B[32m      5\u001B[39m     ds_train.news.iloc[i][\u001B[33m\"\u001B[39m\u001B[33mtitle\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m      6\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m batch[\u001B[33m\"\u001B[39m\u001B[33mhistory\u001B[39m\u001B[33m\"\u001B[39m][\u001B[32m0\u001B[39m].tolist()\n\u001B[32m      7\u001B[39m ])\n\u001B[32m      8\u001B[39m log.info(\u001B[33m\"\u001B[39m\u001B[33mText features shape: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m, txt.shape)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/adv312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/adv312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1515\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1513\u001B[39m worker_id = \u001B[38;5;28mself\u001B[39m._task_info.pop(idx)[\u001B[32m0\u001B[39m]\n\u001B[32m   1514\u001B[39m \u001B[38;5;28mself\u001B[39m._rcvd_idx += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1515\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mworker_id\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/adv312/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1550\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._process_data\u001B[39m\u001B[34m(self, data, worker_idx)\u001B[39m\n\u001B[32m   1548\u001B[39m \u001B[38;5;28mself\u001B[39m._try_put_index()\n\u001B[32m   1549\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[32m-> \u001B[39m\u001B[32m1550\u001B[39m     \u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1551\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/adv312/lib/python3.12/site-packages/torch/_utils.py:750\u001B[39m, in \u001B[36mExceptionWrapper.reraise\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    746\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m    747\u001B[39m     \u001B[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001B[39;00m\n\u001B[32m    748\u001B[39m     \u001B[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001B[39;00m\n\u001B[32m    749\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m750\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[31mRuntimeError\u001B[39m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zliu/miniconda3/envs/adv312/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zliu/miniconda3/envs/adv312/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_1016175/3085113961.py\", line 4, in collate_fn\n    I = torch.stack([b[\"impr_ids\"] for b in batch])\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [5] at entry 0 and [13] at entry 1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = GPTRec(vocab_size=len(ds_train.nid2idx)).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "seq_len = 33"
   ],
   "id": "fcae2b1cc6226d94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   source": [
    "# train\n",
    "num_epochs = 5\n",
    "seq_len = 33\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    tot_loss, tot_acc, tot_samples = 0, 0, 0\n",
    "    pbar = tqdm(loader_train, desc=f\"Epoch {ep}\")\n",
    "    for batch in pbar:\n",
    "        H, I, L = batch[\"history\"].to(device), batch[\"impr_ids\"].to(device), batch[\"impr_lbls\"].to(device)\n",
    "        B = H.size(0)\n",
    "\n",
    "        # Find positive samples\n",
    "        pos_mask = L == 1\n",
    "        pos_imp = I[pos_mask]\n",
    "        # Check if there are any positive samples in the batch\n",
    "        if pos_imp.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        # build input sequence\n",
    "        inp = torch.cat([H, pos_imp.unsqueeze(1)], dim=1)\n",
    "        mask = (inp != 0).long()\n",
    "        logits = model(inp, attention_mask=mask)[:, -1, :]\n",
    "        loss = loss_fn(logits, pos_imp)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc = (preds == pos_imp).sum().item()\n",
    "\n",
    "        tot_loss += loss.item() * B\n",
    "        tot_acc += acc\n",
    "        tot_samples += B\n",
    "\n",
    "        pbar.set_postfix(loss=tot_loss/tot_samples, acc=tot_acc/tot_samples)\n",
    "\n",
    "    log.info(f\"Epoch {ep}: loss={tot_loss/tot_samples:.4f}, acc={tot_acc/tot_samples:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"baseline.pt\")"
   ],
   "id": "d10246832bce829d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
       "model.eval()\n",    "hits, ndcgs = [], []\n",    "\n",    "for batch in loader_val:\n",    "    H, I, L = batch[\"history\"].to(device), batch[\"impr_ids\"].to(device), batch[\"impr_lbls\"].to(device)\n",    "    B = H.size(0)\n",    "\n",    "    # Find positive samples\n",    "    pos_mask = L == 1\n",    "    pos_imp = I[pos_mask]\n",    "        # Check if there are any positive samples in the batch\n",    "        if pos_imp.shape[0] == 0:\n",    "            continue\n",    "\n",    "\n",    "    inp = H\n",    "    mask = (inp != 0).long()\n",    "    logits = model(inp, attention_mask=mask)[:, -1, :]\n",    "    topk = torch.topk(logits, k=10).indices\n",    "\n",    "    for i in range(B):\n",    "        t = pos_imp[i].item()\n",    "        preds = topk[i].tolist()\n",    "        hits.append(int(t in preds))\n",    "        if t in preds:\n",    "            rank = preds.index(t)\n",    "            ndcgs.append(1 / math.log2(rank + 2))\n",    "        else:\n",    "            ndcgs.append(0)\n",    "\n",    "log.info(\"HR@10: %s\", sum(hits) / len(hits))\n",    "log.info(\"NDCG@10: %s\", sum(ndcgs) / len(ndcgs))"   ],
   "id": "6c927131e506df50",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
